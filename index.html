<!-- Page adopted from https://github.com/adjidieng/adjidieng.github.io and https://github.com/jonbarron/website -->

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    a {
      color: #1772d0;
      text-decoration:none;
    }

    em {
      color: #1772d0;
    }

    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }

    td,th,tr,p,a {
      font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 17px;
      font-weight: 300;
    }

    body { 
    	margin:5px 0; 
    	padding:0; 
    	font-size: 100%;
    	font-family: "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
    	color:#FFF;  
    	background-color:#eee;
    	line-height: 1.4em; 
    	/*background : #E4E4E4 url(bg.gif) repeat-x;*/
     	/*background: #E4E4E4 url(bg_light.gif) repeat-x; */
    	background: #FFFFFF url(bc-website-color.gif) repeat-x;
    }
    p { 
    	margin: 0 0 5px 0; 
    	padding: 0; 
    	color: #404241; 
    	background: inherit;
    	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 95%;
      font-weight: 400;
    }

    hr {
      border: 0;
      height: 1px;
      color: #eee;
      background-color: #eee;
    }

    a { 
    	background: inherit; 
    	text-decoration:none;
    	font-size:95%;
      color: #191970;

    }

    a:hover { 
    	background: inherit;
    	text-decoration: none;

    }

    h1 { 
    	padding:0; 
    	margin:0; 
    	color: #434A55; 
    	background: inherit;
    	font-family: serif; 
    	/*font-size: 22px;*/
    	/*letter-spacing: 0px;*/
    }

    h1 a {
    	color: #191970; 
    	background: inherit;
    }

    h2 { 
    	background-color: inherit; 
    	color:#191970; 
    	margin: 10px 20px 10px 0px; 
    	padding:15px 0px 0 0px; 
    	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 18px;
      font-weight: 500;

    }

    h2 a { 
    	/*background-color:#08152E;*/ 
    	/*background-color:#E4E4E4;*/
    	background-color:#FFFFFF;
    }

    ul { 	margin: 0 0 20px 0; 
    	padding : 0; 
    	list-style : none; 
      color: #555;
    }
    	
    li { 
    	float: left;
    	font-weight: bold;
    	margin: 10px 0 8px 0;
    	padding: 0 0 0 5px;
    	font-size: 95%%;
    	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    	font-weight: 400;
      color: #404241;
      width: 100%;

    }

    li a { 
      font-size: 95%;
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-weight: 400;
      color: #191970; }
    li a:hover {text-decoration: none; 
      background: inherit url(select.gif) no-repeat center top;padding: 2px 4px;
    	background-position: 100% 100%;
      color: #DBBC58;
      padding: 4px 8px; 
      background-color:#191970; 
      border-radius: 25px;
      display: run-in;
    }

    strong {
      font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      color: #191970;
    }

    heading {
      font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 23px;
      color: #191970;
    }

    papertitle {
      font-family:  'Titillium Web', Verdana, Helvetica, sans-serif;
      font-size: 17px;
      font-weight: 700
    }

    name {
      font-family:  "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
      font-size: 35px;
      font-weight: bold;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    .some_list { 
    	font-size: 93%;
    	font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    	font-weight: 400;
    	/*float: none;*/
    	list-style-type: circle;
    	padding : 0px; 
    	margin: 0 0 10px 30px; 
      }
  </style>
  <link rel="stylesheet" type="text/css" href="./stylesheet.css">
  <link rel="icon" type="image/png" href="Images/profile_pic.png">
  <title>Jiachen Lu</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Roboto:300,400,500,700,900,100italic,100,300,300italic,400italic,500italic,900italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="70%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <p>
            <name>Jiachen Lu</name>
          </p>
        </td>
      </tr>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
              </p>
              <p>Master Student at Fudan University, Shanghai, China</p>
              <br> 
              <p> I am interested in <strong>Deep Learning</strong>, with a special focus on <strong>Computer Vision</strong> and <strong>Autonomouc Driving</strong>. My latest works focus on <strong>Efficient Transformer Theory</strong> and <strong>Vision-based 3D representation learning</strong>.</p>
              <br>
              <p>
              I received B.Eng. degree in electronic and computer engineering from <a href="https://www.ji.sjtu.edu.cn/">University of Michigan-Shanghai Jiaotong University (UM-SJTU) Joint Institute</a>. I am now a Master student in <a href="https://www.fudan.edu.cn/en/">School of Data Science, Fudan University</a>. During my Master, I conduct my research at <a href="https://fudan-zvg.github.io/">Zhang-vsion Group</a> under the supervision of <a href="https://www.robots.ox.ac.uk/~lz/">Prof. Li Zhang</a> and I do my internship at <a href="http://dev3.noahlab.com.hk/">Noah's Ark Lab</a>.
              </p>
              <br>
              <p style="text-align:center">
                <!-- <a href="CV/CV_Andre_updated_final.pdf">CV</a> &nbsp/&nbsp -->
                <a href="pdf/CV_JiachenLu.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=OECsdBsAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/jiachenlu-2454a7224/"> LinkedIn </a> &nbsp/&nbsp
                <!-- <a href="https://github.com/Andrews2017">Github</a> &nbsp/&nbsp -->
                <a href="mailto: jclu21@m.fudan.edu.cn"> Email to:</a> jclu21/at/m.fudan.edu.cn
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="Images/JiachenLu_CVPR.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="Images/JiachenLu_CVPR.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody>
      </table>
 
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <ul style="list-style-type:circle">
            <li class="some_list">July 2023: My work on <strong>HD-map of Autonomous Driving </strong> "Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach" is accepted as <strong>ICCV 2023 Oral</strong>.</li>
            <li class="some_list">June 2023: My work on <strong>3D Temporal Detection </strong> "SUIT: Learning Significance-guided Information for 3D Temporal Detection" is accepted by <strong>IROS 2023</strong>.</li>
            <li class="some_list">March 2023: My work on <strong>generative perception model</strong> <a href="https://arxiv.org/pdf/2303.11316">"Geneartive Semantic Segmentation"</a> is accepted by <strong>CVPR 2023</strong>.</li>
            <li class="some_list">Jan 2023: My work on <strong>mobile Transformer</strong> <a href="https://openreview.net/pdf?id=-qg8MQNrxZw">"SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation"</a> is accepted by <strong>ICLR 2023</strong>.</li>
            <li class="some_list">Jan 2023: <strong>E2EAD CVPR2023:</strong> 1st workshop on End-to-End Autonomous Driving:
              Perception, Prediction, Planning and Simulation is now open for <a href="https://cmt3.research.microsoft.com/E2EAD2023"><strong>submission</strong></a> on CMT.</li>
            <li class="some_list">Dec 2022: Committee member of <strong>E2EAD CVPR2023</strong> </li>
            <li class="some_list">Dec 2022: My personal citation excceed <strong>1000</strong> on google scholar!</li>
            <li class="some_list">Oct 2022: Awarded 2021-2022 <strong>China National Scholarship</strong>.</li>
          </ul>         
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p> I have my interest in computervision. I have worked on semantic segmentation, Transformer, efficient Transformer, vision-based 3D detection, Bird's Eye View road segmentation.
          </p>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
        <tr>
          <td width="25%">
            <img src='Images/rntr.png' width="280" height="230">
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.pdf">
                <papertitle>Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach</papertitle>
              </a>
              <br>
              <strong>Jiachen Lu</strong>, Renyuan Peng, Xinyue Cai, Hang Xu, Hongyang Li, Feng Wen, Wei Zhang, Li Zhang
              <br>
              <em>ICCV 2023</em> <strong>[Oral]</strong>
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.pdf">Paper</a>/<a href="https://github.com/fudan-zvg/RoadNetworkTRansformer">Code</a>
            </p>
            <p>The extraction of road network is essential for the generation of high-definition maps since it enables the precise localization of road landmarks and their interconnections. However, generating road network poses a significant challenge due to the conflicting underlying combination of Euclidean (eg, road landmarks location) and non-Euclidean (eg, road topological connectivity) structures. Existing methods struggle to merge the two types of data domains effectively, but few of them address it properly. Instead, our work establishes a unified representation of both types of data domain by projecting both Euclidean and non-Euclidean data into an integer series called RoadNet Sequence. Further than modeling an auto-regressive sequence-to-sequence Transformer model to understand RoadNet Sequence, we decouple the dependency of RoadNet Sequence into a mixture of auto-regressive and non-autoregressive dependency. Building on this, our proposed non-autoregressive sequence-to-sequence approach leverages non-autoregressive dependencies while fixing the gap towards auto-regressive dependencies, resulting in success on both efficiency and accuracy. Extensive experiments on nuScenes dataset demonstrate the superiority of RoadNet Sequence representation and the non-autoregressive approach compared to existing state-of-the-art alternatives.
            </p>
          </td>
        </tr>
        <tr>
          <td width="25%">
            <img src='Images/suit.png' width="280" height="150">
          </td>
          <td valign="top" width="75%">
            <p>
              <a href="https://arxiv.org/pdf/2307.01807">
                <papertitle>SUIT: Learning Significance-guided Information for 3D Temporal Detection</papertitle>
              </a>
              <br>
              Zheyuan Zhou, <strong>Jiachen Lu</strong>, Yihan Zeng, Hang Xu, Li Zhang
              <br>
              <em>IROS 2023</em>
            </p>
            <p>3D object detection from LiDAR point cloud is of critical importance for autonomous driving and robotics. While sequential point cloud has the potential to enhance 3D perception through temporal information, utilizing these temporal features effectively and efficiently remains a challenging problem. Based on the observation that the foreground information is sparsely distributed in LiDAR scenes, we believe sufficient knowledge can be provided by sparse format rather than dense maps. To this end, we propose to learn Significance-gUided Information for 3D Temporal detection (SUIT), which simplifies temporal information as sparse features for information fusion across frames. Specifically, we first introduce a significant sampling mechanism that extracts information-rich yet sparse features based on predicted object centroids. On top of that, we present an explicit geometric transformation learning technique, which learns the object-centric transformations among sparse features across frames. We evaluate our method on large-scale nuScenes and Waymo dataset, where our SUIT not only significantly reduces the memory and computation cost of temporal fusion, but also performs well over the state-of-the-art baselines.
            </p>
          </td>
        </tr>
      <tr>
        <td width="25%">
          <img src='Images/gss.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://arxiv.org/pdf/2303.11316">
              <papertitle>Generative Semantic Segmentation</papertitle>
            </a>
            <br>
            Jiaqi Chen, <strong>Jiachen Lu</strong>, Xiatian Zhu, Li Zhang
            <br>
            <em>CVPR 2023</em>
            <br>
            <a href="https://arxiv.org/pdf/2303.11316">Paper</a>/<a href="https://github.com/fudan-zvg/GSS">Code</a>
          </p>
          <p>We present Generative Semantic Segmentation (GSS), a generative learning approach for semantic segmentation. Uniquely, we cast semantic segmentation as an image-conditioned mask generation problem. This is achieved by replacing the conventional per-pixel discriminative learning with a latent prior learning process. Specifically, we model the variational posterior distribution of latent variables given the segmentation mask. To that end, the segmentation mask is expressed with a special type of image (dubbed as maskige). This posterior distribution allows to generate segmentation masks unconditionally. To achieve semantic segmentation on a given image, we further introduce a conditioning network. It is optimized by minimizing the divergence between the posterior distribution of maskige (i.e., segmentation masks) and the latent prior distribution of input training images. Extensive experiments on standard benchmarks show that our GSS can perform competitively to prior art alternatives in the standard semantic segmentation setting, whilst achieving a new state of the art in the more challenging cross-domain setting.</p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/seaformer.png' width="280" height="150">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://openreview.net/pdf?id=-qg8MQNrxZw">
              <papertitle>SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation</papertitle>
            </a>
            <br>
            Qiang Wan, Zilong_Huang, <strong>Jiachen Lu</strong>, Gang YU, Li Zhang
            <br>
            <em>ICLR 2023</em>
            <br>
            <a href="https://openreview.net/pdf?id=-qg8MQNrxZw">Paper</a>/<a href="https://github.com/fudan-zvg/SeaFormer">Code</a>
            <!-- <a href="https://github.com/fudan-zvg/SOFT">Code</a> -->
          </p>
          <p>Since the introduction of Vision Transformers, the landscape of many computer vision tasks (e.g., semantic segmentation), which has been overwhelmingly dominated by CNNs, recently has significantly revolutionized. However, the computational cost and memory requirement render these methods unsuitable on the mobile device, especially for the high resolution per-pixel semantic segmentation task. In this paper, we introduce a new method squeeze-enhanced Axial Transformer (SeaFormer) for mobile semantic segmentation. Specifically, we design a generic attention block characterized by the formulation of squeeze Axial and spatial enhancement. It can be further used to create a family of backbone architectures with superior cost-effectiveness. Coupled with a light segmentation head, we demonstrate state-of-the-art results on the ADE20K, Pascal Context and COCO-stuff datasets. Critically, we beat both the mobile-friendly rivals and Transformer-based counterparts with better performance and lower latency without bells and whistles. Beyond semantic segmentation, we further apply the proposed SeaFormer architecture to image classification problem, demonstrating the potentials of serving as a versatile mobile-friendly backbone.</p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/ego3rt.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://arxiv.org/pdf/2206.04042">
              <papertitle>Learning Ego 3D Representation as Ray Tracing</papertitle>
            </a>
            <br>
            <strong>Jiachen Lu</strong>, Zheyuan Zhou, Xiatian Zhu, Hang Xu, Li Zhang
            <br>
            <em>17th European Conference on Computer Vision (ECCV2022)</em>
            <br>
            <a href="https://arxiv.org/pdf/2206.04042">Paper</a>/
            <a href="https://github.com/fudan-zvg/Ego3RT">Code</a>
          </p>
          <p>A self-driving perception model aims to extract 3D semantic representations from multiple cameras collectively into the bird's-eye-view (BEV) coordinate frame of the ego car in order to ground downstream planner. Existing perception methods often rely on error-prone depth estimation of the whole scene or learning sparse virtual 3D representations without the target geometry structure, both of which remain limited in performance and/or capability. In this paper, we present a novel end-to-end architecture for ego 3D representation learning from an arbitrary number of unconstrained camera views. Inspired by the ray tracing principle, we design a polarized grid of "imaginary eyes" as the learnable ego 3D representation and formulate the learning process with the adaptive attention mechanism in conjunction with the 3D-to-2D projection. Critically, this formulation allows extracting rich 3D representation from 2D images without any depth supervision, and with the built-in geometry structure consistent w.r.t. BEV. Despite its simplicity and versatility, extensive experiments on standard BEV visual tasks (e.g., camera-based 3D object detection and BEV segmentation) show that our model outperforms all state-of-the-art alternatives significantly, with an extra advantage in computational efficiency from multi-task learning.</p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/soft_eg.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://proceedings.neurips.cc/paper/2021/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf">
              <papertitle>SOFT: softmax-free transformer with linear complexity</papertitle>
            </a>
            <br>
            <strong>Jiachen Lu</strong>, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, Li Zhang
            <br>
            <em>NeurIPS2021</em> <strong>[Spotlight]</strong>
            <br>
            <a href="https://proceedings.neurips.cc/paper/2021/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf">Paper</a>/
            <a href="https://github.com/fudan-zvg/SOFT">Code</a>
          </p>
          <p>Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.</p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/setr.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf">
              <papertitle>Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</papertitle>
            </a>
            <br>
            Sixiao Zheng, <strong>Jiachen Lu</strong>, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, Li Zhang
            <br>
            <em>CVPR 2021</em> <strong>[Cited by 2100+]</strong>
            <br>
            <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf">Paper</a>/
            <a href="https://github.com/fudan-zvg/SETR">Code</a>
          </p>
          <p>Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.</p>
        </td>
      </tr>
      <tr>
        <td width="25%">
          <img src='Images/linear_transformer.png' width="280" height="250">
        </td>
        <td valign="top" width="75%">
          <p>
            <a href="https://arxiv.org/pdf/2207.03341.pdf">
              <papertitle>Softmax-free Linear Transformers</papertitle>
            </a>
            <br>
            <strong>Jiachen Lu</strong>, Li Zhang, Junge Zhang, Xiatian Zhu, Hang Xu, Jianfeng Feng
            <br>
            <em>IJCV minor</em>
            <br>
            <a href="https://arxiv.org/pdf/2207.03341.pdf">Paper</a>/
            <a href="https://github.com/fudan-zvg/SOFT">Code</a>
          </p>
          <p>Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by stacked self-attention operations. Employing self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have thus been made in Natural Language Processing. However, an in-depth analysis in this work reveals that they are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in retaining the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Preserving the softmax operation challenges any subsequent linearization efforts. Under this insight, a SOftmax-Free Transformer (abbreviated as SOFT) is proposed for the first time. To eliminate the softmax operator in self-attention, a Gaussian kernel function is adopted to replace the dot-product similarity. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of our approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. <strong>Further, an efficient symmetric normalization is introduced on the low-rank self-attention for enhancing model generalizability and transferability.</strong> Extensive experiments on ImageNet, COCO and ADE20K show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.</p>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Awards and Honors</heading>
          <ul style="list-style-type:circle">
            <li class="some_list">2022: 2021-2022 <strong>China National Scholarship (国家奖学金)</strong>.</li>
            <li class="some_list">2021: <strong>Shanghai Outstanding Graduate (上海市优秀毕业生)</strong>.</li>
            <li class="some_list">2020: 2019-2020 John Wu & Jane Sun Sunshine Scholarship of SJTU.</li>
            <li class="some_list">2019: 3rd Prize of Formula Student Autonomous China.</li>
            <li class="some_list">2019: 2018-2019 John Wu & Jane Sun Sunshine Scholarship of SJTU.</li>
            <li class="some_list">2019: 2018-2019 <strong>China National Scholarship (国家奖学金)</strong>.</li>
            <li class="some_list">2018: 2017-2018 Yuliming Scholarship of SJTU.</li>
          </ul>         
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading>Past News</heading>
            <ul style="list-style-type:circle">
              <li class="some_list">July 2022: My latests work on Transformer theory with respective of <strong>graph spectrum</strong> <a href="https://arxiv.org/pdf/2207.03341">, "Softmax-free Linear Transformers"</a> is now available on arxiv.</li>
              <li class="some_list">July 2022: My paper <a href="https://arxiv.org/pdf/2206.04042">"Learning Ego 3D Representation as Ray Tracing"</a> has been accepted by <a href="https://eccv2022.ecva.net/"><strong>ECCV2022</strong></a>.</li>
              <li class="some_list">April 2022: My talk on <strong>Efficient Transformer</strong> has been published on <a href="https://www.techbeat.net/talk-info?id=650">TechBeat</a>. </li>
              <li class="some_list">Sep 2021: My paper <a href="https://proceedings.neurips.cc/paper/2021/file/b1d10e7bafa4421218a51b1e1f1b0ba2-Paper.pdf">"SOFT: softmax-free transformer with linear complexity"</a> has been accepted as <strong>Spotlight</strong> by <a href="https://nips.cc/Conferences/2021"><strong>NeurIPS2021</strong></a>.</li>
              <li class="some_list">May 2022: My personal citation excceed 500 on google scholar!</li>
              <li class="some_list">Aug 2021: Received a B.Eng. degree from Shanghai Jiaotong University.</li>
              <li class="some_list">Feb 2021: My paper <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf">"Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers"</a> has been accepted by <a href="https://cvpr2021.thecvf.com/"><strong>CVPR2021</strong></a>.</li>
              <li class="some_list">June 2021: Awarded Shanghai Outstanding Graduate.</li>
              <li class="some_list">Dec 2020: Starting my internship at <a href="http://dev3.noahlab.com.hk/">Noah's Ark Lab</a>.</li>
              <li class="some_list">Sep 2019: Awarded 2018-2019 <strong>China National Scholarship</strong>.</li>
            </ul>         
          </td>
        </tr>
        </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Languages</heading>
          <ul style="list-style-type:circle">
            <li class="some_list"> Strong reading, writing, speaking and listening competencies for <strong>Mandarin Chinese</strong> and <strong>English</strong>.</li>
          </ul>         
        </td>
      </tr>
      </table>
    </td>
    </tr>
  </table>
  </body>
</html>
